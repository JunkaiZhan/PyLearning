{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "import socket\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your first web page ---------------------------------------------------------------\n",
    "response = urllib.request.urlopen(\"https://www.python.org\")\n",
    "print(\"The status code of response is >>> \", response.status)\n",
    "print(\"The header of response is >>> \", response.getheaders())\n",
    "print(\"The server of response is >>> \", response.getheader(\"Server\"))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request a web page with argument \"data\" ------------------------------------------------\n",
    "encode   = urllib.parse.urlencode({'Name':'Junkai'}) # Transform the dict to string\n",
    "data     = bytes(encode, encoding = 'utf8')          # Transform the string to byte stream\n",
    "response = urllib.request.urlopen(\"http://httpbin.org/post\", data = data)\n",
    "print(\"The encode is >>> \", encode, \">>> and the type of encode is >>> \", type(encode))\n",
    "print(\"The data in utf8 format is >>> \", data, \" >>> and the type of the data is >>> \", type(data))\n",
    "print(\"The response body is >>> \", response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request a web page with argument \"timeout\" --------------------------------------------\n",
    "try:\n",
    "    response = urllib.request.urlopen(\"http://httpbin.org/get\", timeout = 0.1)\n",
    "except urllib.error.URLError as err:\n",
    "    # err.reason返回的不一定是字符串，可能是一个对象\n",
    "    if isinstance(err.reason, socket.timeout):\n",
    "        print(\"TIME OUT!!!\")\n",
    "# except Exception as exc:\n",
    "    # print(traceback.format_exc())\n",
    "    # print(\"An exception >>> \", exc)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request a web page in a advanced way ------------------------------------------------\n",
    "# 使用Request对象可以更多定义请求的内容和方式\n",
    "\n",
    "request  = urllib.request.Request('https://www.python.org')\n",
    "response = urllib.request.urlopen(request)\n",
    "print(\"The response is >>> \", response.read().decode('utf-8')) # Every string can be specified decode type"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request a web page with all the arguments --------------------------------------------\n",
    "url    = \"http://httpbin.org/post\"\n",
    "header = {\n",
    "    'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',\n",
    "    'Host': 'httpbin.org' \n",
    "}\n",
    "form = {\n",
    "    'name': 'HelloWorld'\n",
    "}\n",
    "data = bytes(urllib.parse.urlencode(form), encoding = 'utf-8')\n",
    "\n",
    "# Method 1\n",
    "# req  = urllib.request.Request(url = url, data = data, headers = header, method = 'POST')\n",
    "\n",
    "# Method 2\n",
    "req = urllib.request.Request(url = url, data = data, method = \"POST\")\n",
    "req.add_header('User-Agent', \"Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)\")\n",
    "\n",
    "response = urllib.request.urlopen(req)\n",
    "print(\"The response is >>> \", response.read().decode('utf-8')) # Every string can be specified decode type"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request a web page in a more advanced way -------------------------------------------------\n",
    "# 在请求网页的时候，不仅仅是要传递表单或者设置请求header，现在有一个更加高级的配置\n",
    "# 可以用于处理管理认证、代理设置、Cookie处理、重定向、错误异常处理等在请求过程的各种功能\n",
    "# 这时候引入urllib下面的各种handler，来帮助我们实现以上功能\n",
    "\n",
    "# >>>>>>>>>> 认证管理 <<<<<<<\n",
    "from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener\n",
    "from urllib.error import URLError\n",
    "\n",
    "url      = \"http://localhost:5000/\"\n",
    "username = \"username\"\n",
    "password = \"password\"\n",
    "\n",
    "p = HTTPPasswordMgrWithDefaultRealm()\n",
    "p.add_password(None, url, username, password)\n",
    "auth_handler = HTTPBasicAuthHandler(p)\n",
    "opener = build_opener(auth_handler)\n",
    "\n",
    "try:\n",
    "    result = opener.open(url)\n",
    "    html = result.read().decode('utf-8')\n",
    "    print(html)\n",
    "except URLError as e:\n",
    "    print(e.reason)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# >>>>>>>>>> 代理设置 <<<<<<<<\n",
    "from urllib.request import ProxyHandler, build_opener\n",
    "from urllib.error import URLError\n",
    "\n",
    "url = \"https://www.baidu.com\"\n",
    "\n",
    "proxy_config = {\n",
    "    \"http\": \"http://127.0.0.1:9999\",\n",
    "    \"https\": \"https://127.0.0.1:9999\"\n",
    "}\n",
    "proxy_handler = ProxyHandler(proxy_config)\n",
    "opener = build_opener(proxy_handler)\n",
    "\n",
    "try:\n",
    "    result = opener.open(url)\n",
    "    html = result.read().decode('utf-8')\n",
    "    print(html)\n",
    "except URLError as e:\n",
    "    print(e.reason)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# >>>>>>>>>> Cookie处理 <<<<<<<<\n",
    "from urllib.request import HTTPCookieProcessor, build_opener\n",
    "import http.cookiejar\n",
    "\n",
    "cookie  = http.cookiejar.CookieJar()\n",
    "\n",
    "# 也可以将cookies保存成文件，不同CookieJar子类用于不用的cookies文件格式\n",
    "# cookie  = http.cookiejar.MozillaCookieJar(\"MozillaCookies.txt\")\n",
    "# cookie  = http.cookiejar.LWPCookieJar(\"LWPCookies.txt\")\n",
    "\n",
    "# 如果已经有了cookies文件，也可以load进来\n",
    "# cookie.load(\"cookies.txt\", ignore_discard = True, ignore_expires = True)\n",
    "\n",
    "handler = HTTPCookieProcessor(cookie)\n",
    "opener  = build_opener(handler)\n",
    "\n",
    "try:\n",
    "    result = opener.open(\"https://www.baidu.com\")\n",
    "    for item in cookie:\n",
    "        print(item.name + \" = \" + item.value)\n",
    "except URLError as e:\n",
    "    print(e.reason)\n",
    "\n",
    "# 如果想要保存文件，需要去掉下一行注释\n",
    "# cookie.save(ignore_discard = True, ignore_expires = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# >>>>>>>>>> 异常处理 <<<<<<<<<<<<\n",
    "from urllib import request, error\n",
    "\n",
    "try:\n",
    "    response = request.urlopen('https://talkchip.com/demo.html')\n",
    "except error.HTTPError as err:\n",
    "    print(err.reason, err.code, err.headers, sep = '\\n')\n",
    "except error.URLError as err:\n",
    "    print(err.reason)\n",
    "except Exception as err:\n",
    "    print(traceback.format_exc())\n",
    "else:\n",
    "    print('Request Successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using urllib to parse URL -------------------------------------------------------------\n",
    "# 下面的部分是解析链接的各种例程\n",
    "\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urlunparse\n",
    "from urllib.parse import urlsplit\n",
    "from urllib.parse import urlunsplit\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# One completed URL is scheme://netloc/path;params?query#fragment\n",
    "url = 'http://www.baidu.com/index.html;user?id=5#comment'\n",
    "\n",
    "def print_parse_result(result):\n",
    "    \n",
    "    if isinstance(result, urllib.parse.ParseResult):\n",
    "        print(\"\\nresult.scheme: \", result.scheme)\n",
    "        print(\"result.netloc: \", result.netloc)\n",
    "        print(\"result.path: \", result.path)\n",
    "        print(\"result.params: \", result.params)\n",
    "        print(\"result.query: \", result.query)\n",
    "        print(\"result.fragment: \", result.fragment, \"\\n\")\n",
    "\n",
    "# >>>>>> 链接解析拆分 <<<<<<<<\n",
    "result = urlparse(url)\n",
    "print(type(result), result, sep = \"\\n\")\n",
    "print_parse_result(result)\n",
    "\n",
    "result = urlparse(url, allow_fragments = False)\n",
    "print(type(result), result, sep = \"\\n\")\n",
    "print_parse_result(result)\n",
    "\n",
    "result = urlsplit(url) # 跟urlparse有一点区别\n",
    "print(type(result), result, sep = \"\\n\")\n",
    "print_parse_result(result)\n",
    "\n",
    "# >>>>>> 链接合并，严格要求参数数量 <<<<<<\n",
    "data = ['http', 'www.baidu.com', \"index.html\", \"user\", \"a=6\", \"comment\"]\n",
    "print(urlunparse(data))\n",
    "\n",
    "data = ['http', 'www.baidu.com', 'index.html', 'a=6', 'comment']\n",
    "print(urlunsplit(data))\n",
    "\n",
    "# urljoin函数会解析第一个参数的scheme, netloc 和 path\n",
    "# 然后对第二个参数进行缺失部分的补充，即缺失则补充，不缺失不替换\n",
    "print(urljoin('http://www.baidu.com', 'FAQ.html'))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlencode\n",
    "from urllib.parse import parse_qs\n",
    "from urllib.parse import parse_qsl\n",
    "\n",
    "# >>>>>>> 在字典中定义参数，然后拼接到url中作为GET请求的参数 <<<<<<\n",
    "params = {\n",
    "    'name': 'talkchip',\n",
    "    'age': '2'\n",
    "}\n",
    "base_url = 'http://www.baidu.com?'\n",
    "new_url  = base_url + urlencode(params)\n",
    "print(new_url)\n",
    "\n",
    "# >>>>>> 反向操作，将参数解析成字典和元组列表 <<<<<<<\n",
    "query = 'name=talkchip&age=2'\n",
    "print('parse_qs result >>> ', parse_qs(query))\n",
    "print('parse_qsl result >>> ', parse_qsl(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote\n",
    "from urllib.parse import unquote\n",
    "\n",
    "# >>>>>> 将内容转化为URL编码及其反向操作 <<<<<<<\n",
    "keyword = '铁观音'\n",
    "url_url = 'https://www.baidu.com/s?wd=' + quote(keyword)\n",
    "zh_url  = unquote(url_url)\n",
    "print(url, zh_url, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 关于robots.txt的一些操作，通过解析该文件，判断哪些网页可以爬取\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from urllib import request, error\n",
    "\n",
    "# Method 1 \n",
    "rp = RobotFileParser('http://www.jianshu.com/robots.txt')\n",
    "rp.read() # 执行读取和分析操作\n",
    "print(\"Method 1 >>> \", rp.can_fetch('*', 'http://www.jianshu.com/p/b67554025d7d'))\n",
    "print(\"Method 1 >>> \", rp.can_fetch('*', 'http://www.jianshu.com/search?q=python&page=1&type=collections'))\n",
    "\n",
    "# Method 2\n",
    "rp = RobotFileParser()\n",
    "# rp.set_url('http://www.jianshu.com/robots.txt')\n",
    "try:\n",
    "    rp.parse(request.urlopen('http://wwww.taobao.com/robots.txt').read().decode('utf-8').split('\\n'))\n",
    "    print(\"Method 2 >>> \", rp.can_fetch('*', 'http://www.jianshu.com/p/b67554025d7d'))\n",
    "    print(\"Method 2 >>> \", rp.can_fetch('*', 'http://www.jianshu.com/search?q=python&page=1&type=collections'))\n",
    "except error.HTTPError as err:\n",
    "    print(err.reason, err.code, err.headers, sep = '\\n')\n",
    "except error.URLError as err:\n",
    "    print(err.reason)\n",
    "except Exception as err:\n",
    "    print(traceback.format_exc())\n",
    "else:\n",
    "    print('Request Successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}